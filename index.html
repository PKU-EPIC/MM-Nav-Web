<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="MM-Nav">
  <!-- <meta name="keywords" content="VLN, VLA model, Foundation Model"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYH3N96LN5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-ZYH3N96LN5');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/slick.css">
  <link rel="stylesheet" href="./static/css/slick-theme.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/slick.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- Swiper CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.css" />

  <!-- Swiper JS -->
  <script src="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.js"></script>

</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">MM-Nav: Multi-View VLA Model for Robust Visual
              Navigation via Multi-Expert Learning</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/Tranxray" target="_blank">Tianyu Xu</a><sup>1,</sup>*
              </span> &nbsp;
              <span class="author-block">
                <a href="https://github.com/187370" target="_blank">Jiawei Chen</a><sup>1,</sup>*
              </span>&nbsp;
              <span class="author-block">
                <a href="https://jzhzhang.github.io" target="_blank">Jiazhao
                  Zhang</a><sup>1,2,</sup>*
              </span>&nbsp;
              <span class="author-block">
                <a href="https://zhangwenyao1.github.io" target="_blank">Wenyao
                  Zhang</a><sup>2, 3</sup>
              </span>&nbsp;
              <span class="author-block">
                <a href="https://qizekun.github.io" target="_blank">Zekun Qi</a><sup>2, 4</sup>
              </span>&nbsp;
              <span class="author-block">
                <a href="https://github.com/lpercc" target="_blank">Minghan Li</a><sup>2</sup>
              </span>&nbsp;
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en" target="_blank">Zhizheng
                  Zhang</a><sup>2,5,†</sup>
              </span>&nbsp;
              <span class="author-block">
                <a href="https://hughw19.github.io" target="_blank">He Wang</a><sup>1,2,5,†</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Peking University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>2</sup>Galbot&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>3</sup>Shanghai Jiao Tong University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>4</sup>Tsinghua University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>5</sup>BAAI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              </span>
            </div>
            
            <div class="is-size-5 publication-authors">
              *Joint First Author&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;†Corresponding Author
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
            
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <span class="link-block">
                  <a  target="_blank" class="external-link button is-normal is-rounded is-light">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Environments</span>
                  </a>
                </span>
                
                <span class="link-block">
                  <a  target="_blank" class="external-link button is-normal is-rounded is-light">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>VLA Model</span>
                  </a>
                </span>
            
              </div>
            </div>
            <!-- <div class="text" style="font-size: 20px;">
                            <b>Conference on Robot Learning <i>(CoRL 2025)</i></b>
                        </div> -->
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop is-centered has-text-justified is-size-5">
      <div class="hero-body">
        <img src="./static/images/pipeline.png" alt="teaser Image" width="100%">
        <p>
          We introduce <strong>MM-Nav</strong>, as a multi-view VLA (with 360° observation) based on pretrained large language models and visual
          foundation models. For large-scale navigation data, we collect expert data from three reinforcement learning (RL)
          experts trained with privileged depth information in three challenging tailor-made environments for different navigation
          capabilities: <em>reaching, squeezing, and avoiding</em>.
        </p>
  
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <video poster="" id="video1" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/wire.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video poster="" id="video2" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/dynamic.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video poster="" id="video3" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/dynwire_2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video poster="" id="video4" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/human.mp4" type="video/mp4">
            </video>
          </div>
          
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Visual navigation policy is widely regarded as a promising direction, as it mimics
              humans by using egocentric visual
              observations for navigation. However, optical information of visual observations is
              difficult to be explicitly modeled
              like LiDAR point clouds or depth map, which subsequently requires intelligent models and
              large-scale data. To this end,
              we propose to leverage the intelligence of Vision-Language-Action (VLA) model to learn
              diverse navigation capabilities
              from synthetic expert data in a teachers-student manner. Specifically, we implement the
              VLA model, MM-Nav, as a
              multi-view VLA (with 360° observation) based on pretrained large language models and
              visual foundation models. For
              large-scale navigation data, we collect expert data from three reinforcement learning
              (RL) experts trained with
              privileged depth information in three challenging tailor-made environments for different
              navigation capabilities:
              'reaching', 'squeezing', 'avoiding'. We iteratively train our VLA model using data
              collected online from RL experts,
              where the training ratio is dynamically balanced based on performance on individual
              capabilities. Through extensive
              experiments in synthetic environments, we demonstrate that our model achieves strong
              generalization capability.
              Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating
              the synergistic effect of
              integrating multiple capabilities. Extensive real-world experiments further confirm the
              effectiveness of our method.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <h2 class="has-text-centered is-size-5">
          Our pipeline comprises two steps: (1) training of multiple RL experts with different capabilities and <br> initial VLA
          finetuning, (2) teachers-student online training iteration between RL experts and VLA.<br>
          <br>
        </h2>
        <div class="has-text-centered">
          <h2 class="title is-5 has-text-centered">Stage 1</h2>
          <h2 class="subtitle has-text-centered">
            We first train RL experts in different environments and collect<br>
            successful trajectories for VLA fine-tuning.
          </h2>
          <!-- <video poster="static/images/strategy1.png" id="video1" controls muted loop width="640" preload="metadata">
            <source src="static/videos/strategy1.mp4" type="video/mp4">
          </video> -->
          <div class="text-image-container title is-3">
            <img src="static/images/strategy1.png" style="width:50%; height:auto;">
          </div>

        </div>
        <div class="has-text-centered">
          <h2 class="title is-5 has-text-centered">
            <br>
            Stage 2
          </h2>
          <h2 class="subtitle has-text-centered">
            We then collect RL expert data online based on VLA observations<br>
            and dynamically balance the training data ratio.
          </h2>
          <!-- <video poster="static/images/strategy2.png" id="video1" controls muted loop width="640" preload="metadata">
            <source src="static/videos/strategy2.mp4" type="video/mp4">
          </video> -->
            <!-- <figure class="image" style="width: 550px;">
              <img src="static/images/strategy2.png" style="width:100%; height:auto;">
            </figure> -->
          <div class="text-image-container title is-3">
            <img src="static/images/strategy2.png" style="width:50%; height:auto;">
          </div>


        </div>
        <div class="has-text-centered">
          <h2 class="subtitle has-text-centered">
            <br>
            The second stage is repeated iteratively, where the training data ratio is balanced dynamically.
          </h2>
          <div class="columns is-centered">
            <div class="column is-narrow">
              <figure class="image" style="width: 550px;">
                <img src="static/images/performance.png" style="width:100%; height:auto;">
              </figure>
            </div>
            <div class="column is-narrow">
              <figure class="image" style="width: 550px;">
                <img src="static/images/proportion.png" style="width:100%; height:auto;">
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <h3 class="title is-3 is-centered has-text-centered">
        <span style="vertical-align: middle;">Simulation Scenes</span>
      </h3>
      <h2 class="has-text-centered is-size-5">
        For each expert, we run 64 parallel robots in their respective simulation environments,<br> recording the observations,
        goals, and corresponding expert actions.
      </h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-narrow">
            <video poster="" id="item1" autoplay controls muted loop playsinline width="400">
              <source src="./static/videos/reach.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              Reaching Scene
            </h2>
          </div>
          <div class="column is-narrow">
            <video poster="" id="item2" autoplay controls muted loop playsinline width="400">
              <source src="./static/videos/pillar.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              Squeezing Scene
            </h2>
          </div>
          <div class="column is-narrow">
            <video poster="" id="item3" autoplay controls muted loop playsinline width="400">
              <source src="./static/videos/dyn.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              Avoiding Scene
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <video poster="" id="video1" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/dynwire_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video poster="" id="video2" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/stick.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video poster="" id="video3" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/cluttered.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video poster="" id="video4" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/corridor.mp4" type="video/mp4">
            </video>
          </div>
  
        </div>
      </div>
    </div>
  </section>

  <script>
    document.addEventListener('DOMContentLoaded', function () {
      const swiper1 = new Swiper('.swiper1', {
        slidesPerView: 3,
        spaceBetween: 20,
        loop: true,
        autoplay: {
          delay: 2000,
          disableOnInteraction: false,
          pauseOnMouseEnter: true
        },
        navigation: false,
        pagination: false,
      });
    });
  </script>

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@misc{xu2025mmnav,
      title={MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning},
      author={Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, He Wang},
      year= {2025},
      }</code></pre>
    </div>
  </section> -->

</body>

</html>

<script>
  const swiper = new Swiper('.swiper', {
    slidesPerView: 4,
    spaceBetween: 10,
    loop: true,
    autoplay: {
      delay: 2000,
      disableOnInteraction: false,
      pauseOnMouseEnter: true
    },
    navigation: {
      //   nextEl: '.swiper-button-next',
      prevEl: '.swiper-button-prev'
    },
    navigation: {
      nextEl: '.swiper-button-next',
      //   prevEl: '.swiper-button-prev'
    },
    pagination: false,
  });
</script>